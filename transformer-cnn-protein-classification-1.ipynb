{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12930228,"sourceType":"datasetVersion","datasetId":8182117}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nimport os\nfrom transformers import TFEsmModel, EsmTokenizer\n\n# Check available data files\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-01T13:55:46.871647Z","iopub.execute_input":"2025-09-01T13:55:46.871922Z","iopub.status.idle":"2025-09-01T13:56:18.687040Z","shell.execute_reply.started":"2025-09-01T13:55:46.871880Z","shell.execute_reply":"2025-09-01T13:56:18.686231Z"}},"outputs":[{"name":"stderr","text":"2025-09-01 13:55:50.166846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756734950.526875      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756734950.638335      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/pdb14189/PDB14189_N.txt\n/kaggle/input/pdb14189/PDB14189_P.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"RANDOM_SEED=229\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T13:56:18.688787Z","iopub.execute_input":"2025-09-01T13:56:18.689847Z","iopub.status.idle":"2025-09-01T13:56:18.695154Z","shell.execute_reply.started":"2025-09-01T13:56:18.689813Z","shell.execute_reply":"2025-09-01T13:56:18.694336Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def load_fasta_data(positive_file, negative_file):\n    \"\"\"Load protein sequences from FASTA files\"\"\"\n    def parse_fasta(file_path):\n        sequences = []\n        current_seq = \"\"\n        with open(file_path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if line.startswith('>'):\n                    if current_seq:\n                        sequences.append(current_seq)\n                    current_seq = \"\"\n                else:\n                    current_seq += line\n        if current_seq:\n            sequences.append(current_seq)\n        return sequences\n    \n    positive_sequences = parse_fasta(positive_file)\n    negative_sequences = parse_fasta(negative_file)\n    positive_labels = [1] * len(positive_sequences)\n    negative_labels = [0] * len(negative_sequences)\n    all_sequences = positive_sequences + negative_sequences\n    all_labels = positive_labels + negative_labels\n    \n    print(f\"Loaded {len(positive_sequences)} positive and {len(negative_sequences)} negative sequences\")\n    print(f\"Total sequences: {len(all_sequences)}\")\n    return all_sequences, all_labels\n\ndef evaluate_model(model, test_data, y_test):\n    \"\"\"Comprehensive model evaluation for transformer model\"\"\"\n    if isinstance(test_data, list):  # For transformer model with multiple inputs\n        y_pred_proba = model.predict(test_data)\n    else:\n        y_pred_proba = model.predict(test_data)\n    \n    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    auc = roc_auc_score(y_test, y_pred_proba)\n    \n    print(f\"Test Accuracy: {accuracy:.4f}\")\n    print(f\"Test Precision: {precision:.4f}\")\n    print(f\"Test Recall: {recall:.4f}\")\n    print(f\"Test F1-Score: {f1:.4f}\")\n    print(f\"Test AUC-ROC: {auc:.4f}\")\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred, target_names=['Non-DNA-binding', 'DNA-binding']))\n    \n    print(\"\\nConfusion Matrix:\")\n    print(confusion_matrix(y_test, y_pred))\n    \n    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1, 'auc': auc}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T13:56:18.695941Z","iopub.execute_input":"2025-09-01T13:56:18.696221Z","iopub.status.idle":"2025-09-01T13:56:18.721339Z","shell.execute_reply.started":"2025-09-01T13:56:18.696196Z","shell.execute_reply":"2025-09-01T13:56:18.720615Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class PretrainedTransformerCNN:\n    def __init__(self, transformer_model=\"facebook/esm2_t33_650M_UR50D\", \n                 max_seq_len=1024, freeze_transformer=True):\n        self.transformer_model = transformer_model\n        self.max_seq_len = max_seq_len\n        self.freeze_transformer = freeze_transformer\n        \n        print(f\"Loading transformer model: {transformer_model}\")\n        self.tokenizer = EsmTokenizer.from_pretrained(transformer_model)\n        self.transformer = TFEsmModel.from_pretrained(transformer_model)\n        \n        if freeze_transformer:\n            for layer in self.transformer.layers:\n                layer.trainable = False\n            print(\"Transformer weights frozen for feature extraction\")\n    \n    def build_model(self):\n        \"\"\"Build the Transformer → CNN → Classification pipeline\"\"\"\n        input_ids = Input(shape=(self.max_seq_len,), dtype=tf.int32, name='input_ids')\n        attention_mask = Input(shape=(self.max_seq_len,), dtype=tf.int32, name='attention_mask')\n        \n        # Wrap transformer call in Lambda layer\n        def transformer_call(inputs):\n            input_ids, attention_mask = inputs\n            return self.transformer(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n        \n        sequence_embeddings = Lambda(transformer_call)([input_ids, attention_mask])\n        \n        # Multi-scale CNN\n        conv_layers = []\n        for kernel_size, filters in [(3, 256), (5, 256), (7, 256)]:\n            conv = Conv1D(filters=filters, kernel_size=kernel_size, padding='same', activation='relu')(sequence_embeddings)\n            conv = BatchNormalization()(conv)\n            conv = MaxPooling1D(pool_size=2)(conv)\n            conv_layers.append(conv)\n        \n        cnn_features = Concatenate(axis=-1)(conv_layers)\n        cnn_features = Conv1D(512, 3, padding='same', activation='relu')(cnn_features)\n        cnn_features = BatchNormalization()(cnn_features)\n        cnn_features = Dropout(0.2)(cnn_features)\n        cnn_features = Conv1D(256, 3, padding='same', activation='relu')(cnn_features)\n        cnn_features = BatchNormalization()(cnn_features)\n        cnn_features = GlobalMaxPooling1D()(cnn_features)\n        \n        # Classification head\n        x = Dense(512, activation='relu')(cnn_features)\n        x = Dropout(0.3)(x)\n        x = Dense(256, activation='relu')(x)\n        x = Dropout(0.2)(x)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(0.1)(x)\n        outputs = Dense(1, activation='sigmoid')(x)\n        \n        model = Model(inputs=[input_ids, attention_mask], outputs=outputs, name='TransformerCNN')\n        return model\n\n    \n    def preprocess_sequences(self, sequences):\n        \"\"\"Preprocess protein sequences for the transformer\"\"\"\n        spaced_sequences = [' '.join(seq) for seq in sequences]\n        encoded = self.tokenizer(spaced_sequences, padding=True, truncation=True, \n                               max_length=self.max_seq_len, return_tensors=\"tf\")\n        return {'input_ids': encoded['input_ids'], 'attention_mask': encoded['attention_mask']}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T13:56:18.722053Z","iopub.execute_input":"2025-09-01T13:56:18.722373Z","iopub.status.idle":"2025-09-01T13:56:18.749615Z","shell.execute_reply.started":"2025-09-01T13:56:18.722345Z","shell.execute_reply":"2025-09-01T13:56:18.748946Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load full dataset for transformer model\nsequences, labels = load_fasta_data(\n    positive_file='/kaggle/input/pdb14189/PDB14189_P.txt',\n    negative_file='/kaggle/input/pdb14189/PDB14189_N.txt'\n)\n\nprint(f\"Using dataset: {len(sequences)} sequences for Transformer CNN model\")\ny_full = np.array(labels)\nprint(f\"Positive samples: {np.sum(y_full)}, Negative samples: {len(y_full) - np.sum(y_full)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T13:56:18.751430Z","iopub.execute_input":"2025-09-01T13:56:18.751657Z","iopub.status.idle":"2025-09-01T13:56:18.910413Z","shell.execute_reply.started":"2025-09-01T13:56:18.751636Z","shell.execute_reply":"2025-09-01T13:56:18.909675Z"}},"outputs":[{"name":"stdout","text":"Loaded 7129 positive and 7060 negative sequences\nTotal sequences: 14189\nUsing dataset: 14189 sequences for Transformer CNN model\nPositive samples: 7129, Negative samples: 7060\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Create transformer model\ntransformer_cnn = PretrainedTransformerCNN()\ntransformer_model = transformer_cnn.build_model()\n\ntransformer_model.compile(\n    optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-5),\n    loss='binary_crossentropy',\n    metrics=['accuracy', 'precision', 'recall']\n)\n\n# Preprocess full dataset\nprint(\"Preprocessing sequences...\")\ntransformer_data = transformer_cnn.preprocess_sequences(sequences)\n\n# Split data\nindices = np.arange(len(sequences))\ntrain_idx, temp_idx = train_test_split(indices, test_size=0.4, random_state=RANDOM_SEED, stratify=y_full)\nval_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=RANDOM_SEED, stratify=y_full[temp_idx])\n\ndef safe_gather(tensor, indices):\n    \"\"\"Safely gather elements from tensor using numpy indices\"\"\"\n    if isinstance(indices, np.ndarray):\n        indices = tf.convert_to_tensor(indices, dtype=tf.int32)\n    return tf.gather(tensor, indices)\n\n# Create datasets using tf.gather\ntrain_data = [\n    safe_gather(transformer_data['input_ids'], train_idx), \n    safe_gather(transformer_data['attention_mask'], train_idx)\n]\nval_data = [\n    safe_gather(transformer_data['input_ids'], val_idx), \n    safe_gather(transformer_data['attention_mask'], val_idx)\n]\ntest_data = [\n    safe_gather(transformer_data['input_ids'], test_idx), \n    safe_gather(transformer_data['attention_mask'], test_idx)\n]\n\ny_train_full, y_val_full, y_test_full = y_full[train_idx], y_full[val_idx], y_full[test_idx]\n\nprint(f\"Training set: {len(train_idx)} samples\")\nprint(f\"Validation set: {len(val_idx)} samples\") \nprint(f\"Test set: {len(test_idx)} samples\")\n# Train transformer model\ntransformer_callbacks = [\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-7, verbose=1),\n    tf.keras.callbacks.ModelCheckpoint('best_transformer_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T13:56:18.911154Z","iopub.execute_input":"2025-09-01T13:56:18.911415Z","iopub.status.idle":"2025-09-01T13:57:19.774068Z","shell.execute_reply.started":"2025-09-01T13:56:18.911390Z","shell.execute_reply":"2025-09-01T13:57:19.773245Z"}},"outputs":[{"name":"stdout","text":"Loading transformer model: facebook/esm2_t33_650M_UR50D\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa496b041a31459dbd9231a5c3632759"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71784012009f470cbf1c94541b705bd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"686cbad2edaf41cf985f291d8ab9576f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"406ae4d68bf84352943b1f5b70a95aed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.61G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bd29e9dd53d442384802c9c6bb46022"}},"metadata":{}},{"name":"stderr","text":"I0000 00:00:1756734993.133049      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1756734993.133983      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFEsmModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'esm.embeddings.position_ids']\n- This IS expected if you are initializing TFEsmModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFEsmModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFEsmModel were not initialized from the PyTorch model and are newly initialized: ['esm.pooler.dense.weight', 'esm.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Transformer weights frozen for feature extraction\nPreprocessing sequences...\nTraining set: 8513 samples\nValidation set: 2838 samples\nTest set: 2838 samples\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(\"=== Training Transformer CNN Model ===\")\n\ntransformer_history = transformer_model.fit(\n    train_data, y_train_full,\n    batch_size=8,\n    epochs=20,\n    validation_data=(val_data, y_val_full),\n    callbacks=transformer_callbacks,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T13:57:19.774860Z","iopub.execute_input":"2025-09-01T13:57:19.775222Z","execution_failed":"2025-09-01T14:05:29.863Z"}},"outputs":[{"name":"stdout","text":"=== Training Transformer CNN Model ===\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1756735101.175811     204 service.cc:148] XLA service 0x7f4a18010de0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1756735101.177224     204 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1756735101.177251     204 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nW0000 00:00:1756735103.737201     204 assert_op.cc:38] Ignoring Assert operator TransformerCNN_1/lambda_1/tf_esm_model/esm/embeddings/assert_less/Assert/Assert\nI0000 00:00:1756735109.193140     204 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1756735125.122523     204 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 106/1065\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:01:15\u001b[0m 4s/step - accuracy: 0.5453 - loss: 1.0421 - precision: 0.5450 - recall: 0.5687","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"print(\"=== Transformer CNN Model Results ===\")\ntransformer_results = evaluate_model(transformer_model, test_data, y_test_full)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-01T14:05:29.864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Save transformer results\n# with open('transformer_cnn_results.pkl', 'wb') as f:\n#     pickle.dump(transformer_results, f)\n\n# print(\"\\nTransformer results saved to 'transformer_cnn_results.pkl'\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-01T14:05:29.864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Load simple CNN results for comparison\n# try:\n#     import pickle\n#     with open('simple_cnn_results.pkl', 'rb') as f:\n#         simple_results = pickle.load(f)\n    \n#     print(\"\\n=== Model Comparison ===\")\n#     print(\"Simple CNN+BiLSTM vs Transformer CNN\")\n#     print(f\"Accuracy: {simple_results['accuracy']:.4f} vs {transformer_results['accuracy']:.4f}\")\n#     print(f\"Precision: {simple_results['precision']:.4f} vs {transformer_results['precision']:.4f}\")\n#     print(f\"Recall: {simple_results['recall']:.4f} vs {transformer_results['recall']:.4f}\")\n#     print(f\"F1-Score: {simple_results['f1_score']:.4f} vs {transformer_results['f1_score']:.4f}\")\n#     print(f\"AUC-ROC: {simple_results['auc']:.4f} vs {transformer_results['auc']:.4f}\")\n    \n# except FileNotFoundError:\n#     print(\"CNN results not found\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-01T14:05:29.864Z"}},"outputs":[],"execution_count":null}]}